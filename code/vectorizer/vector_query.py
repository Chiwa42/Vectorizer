import os
import sys
import logging
import argparse
from typing import List, Dict, Any
from datetime import datetime
import json

# Hugging Faceå’Œå‘é‡è³‡æ–™åº«ç›¸é—œ
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_chroma import Chroma
from langchain.schema import Document

# å°å…¥ Reranker ç›¸é—œåº«
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# è¨­å®š logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class VectorQuerySystem:
    """
    å‘é‡è³‡æ–™åº«æŸ¥è©¢ç³»çµ±ï¼Œå¢åŠ  Reranker é€²è¡ŒäºŒæ¬¡æ’åº
    """
    
    def __init__(self, 
                 chroma_db_dir: str = "../vector_dbs/dev_dbs/",
                 embedding_model_name: str = "BAAI/bge-m3",
                 reranker_model_name: str = "BAAI/bge-reranker-v2-m3",
                 model_kwargs: dict = {'device': 'cuda:0'},
                 encode_kwargs: dict = {'normalize_embeddings': True},
                 query_instruction = "ç‚ºé€™å€‹å¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨æ–¼æª¢ç´¢ç›¸é—œæ–‡ç« ï¼š"):
        """
        åˆå§‹åŒ–æŸ¥è©¢ç³»çµ±
        
        Args:
            chroma_db_dir: å‘é‡è³‡æ–™åº«ç›®éŒ„è·¯å¾‘
            embedding_model_name: åµŒå…¥æ¨¡å‹è·¯å¾‘æˆ–åç¨±
            reranker_model_name: Reranker æ¨¡å‹è·¯å¾‘æˆ–åç¨±
            model_kwargs: æ¨¡å‹åƒæ•¸
            encode_kwargs: ç·¨ç¢¼åƒæ•¸
        """
        self.chroma_db_dir = chroma_db_dir
        self.tokenizer = None
        self.reranker_model = None
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

        # æª¢æŸ¥è³‡æ–™åº«æ˜¯å¦å­˜åœ¨
        if not os.path.exists(chroma_db_dir):
            raise FileNotFoundError(f"å‘é‡è³‡æ–™åº«ç›®éŒ„ä¸å­˜åœ¨: {chroma_db_dir}")

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆå¿…é ˆä½¿ç”¨èˆ‡å»ºç«‹è³‡æ–™åº«æ™‚ç›¸åŒçš„æ¨¡å‹ï¼‰
        try:
            self.embeddings = HuggingFaceBgeEmbeddings(
                model_name=embedding_model_name,
                model_kwargs=model_kwargs,
                encode_kwargs=encode_kwargs,
                query_instruction=query_instruction
            )
            logging.info("âœ… åµŒå…¥æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        except Exception as e:
            logging.error(f"âŒ åµŒå…¥æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise
            
        # è¼‰å…¥ Reranker æ¨¡å‹
        try:
            logging.info(f"ğŸš€ æ­£åœ¨è¼‰å…¥ Reranker æ¨¡å‹: {reranker_model_name}...")
            self.tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)
            self.reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name)
            self.reranker_model.to(self.device).eval()
            logging.info("âœ… Reranker æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        except Exception as e:
            logging.error(f"âŒ Reranker æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            self.reranker_model = None
            logging.warning("âš ï¸ Reranker æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œå°‡é€€å›å–®ç´”å‘é‡æœç´¢æ¨¡å¼")
            
        # è¼‰å…¥å‘é‡è³‡æ–™åº«
        try:
            self.vectordb = Chroma(
                persist_directory=chroma_db_dir,
                embedding_function=self.embeddings
            )
            
            collection_count = self.vectordb._collection.count()
            if collection_count == 0:
                logging.warning("âš ï¸ å‘é‡è³‡æ–™åº«æ˜¯ç©ºçš„ï¼Œè«‹å…ˆåŸ·è¡Œæ–‡æª”å‘é‡åŒ–")
            else:
                logging.info(f"âœ… å‘é‡è³‡æ–™åº«è¼‰å…¥æˆåŠŸï¼ŒåŒ…å« {collection_count} å€‹æ–‡æª”ç‰‡æ®µ")
                
        except Exception as e:
            logging.error(f"âŒ å‘é‡è³‡æ–™åº«è¼‰å…¥å¤±æ•—: {e}")
            raise

    def search(self, query: str, top_k: int = 100, initial_k: int = 1000) -> List[Dict[str, Any]]:
        """
        åŸ·è¡Œé›™éšæ®µèªç¾©æœç´¢
        
        Args:
            query: æŸ¥è©¢å•å¥æˆ–é—œéµå­—
            top_k: æœ€çµ‚è¿”å›çš„çµæœæ•¸é‡ï¼ˆé‡æ’åºå¾Œï¼‰
            initial_k: åˆå§‹å‘é‡æœç´¢çš„å€™é¸é›†æ•¸é‡ï¼ˆé‡æ’åºå‰ï¼‰
            
        Returns:
            æœç´¢çµæœåˆ—è¡¨
        """
        try:
            logging.info(f"ğŸ” åŸ·è¡Œæœç´¢æŸ¥è©¢: '{query}'")
            
            # --- ç¬¬ä¸€éšæ®µï¼šå‘é‡ç›¸ä¼¼åº¦æœç´¢ (Retrieval) ---
            initial_results = self.vectordb.similarity_search_with_score(query, k=initial_k)
            
            # è™•ç†çµæœ
            candidates = []
            seen_content = set()
            for doc, score in initial_results:
                content_hash = hash(doc.page_content[:100])
                if content_hash in seen_content:
                    continue
                seen_content.add(content_hash)
                candidates.append({
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'distance_score': score
                })
            
            if not candidates:
                logging.warning("âš ï¸ å‘é‡æœç´¢æœªæ‰¾åˆ°ä»»ä½•çµæœ")
                return []
                
            logging.info(f"âœ¨ ç¬¬ä¸€éšæ®µæ‰¾åˆ° {len(candidates)} å€‹å€™é¸çµæœ")

            # --- ç¬¬äºŒéšæ®µï¼šé‡æ’åº (Reranking) ---
            if self.reranker_model:
                # å°‡æŸ¥è©¢èˆ‡å€™é¸æ–‡æª”å…§å®¹é…å°
                pairs = [[query, candidate['content']] for candidate in candidates]

                # ä½¿ç”¨ Reranker é€²è¡ŒäºŒæ¬¡è©•åˆ†
                with torch.no_grad():
                    inputs = self.tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}
                    scores = self.reranker_model(**inputs, return_dict=True).logits.view(-1, ).float()

                # å°‡åˆ†æ•¸åŠ å…¥åˆ°å€™é¸çµæœä¸­
                for i, score in enumerate(scores):
                    candidates[i]['rerank_score'] = score.item()
                
                # æ ¹æ“š Reranker åˆ†æ•¸é€²è¡Œæ’åºï¼ˆåˆ†æ•¸è¶Šé«˜è¶Šå¥½ï¼‰
                candidates.sort(key=lambda x: x['rerank_score'], reverse=True)
                
                logging.info("âœ… Reranker é‡æ’åºæˆåŠŸ")
            else:
                # å¦‚æœ Reranker è¼‰å…¥å¤±æ•—ï¼Œå‰‡é€€å›ä½¿ç”¨åˆå§‹çš„è·é›¢åˆ†æ•¸æ’åº
                candidates.sort(key=lambda x: x['distance_score'])
                logging.warning("âš ï¸ ç”±æ–¼ Reranker è¼‰å…¥å¤±æ•—ï¼Œçµæœä½¿ç”¨åˆå§‹è·é›¢åˆ†æ•¸æ’åº")

            # å–å‡ºæœ€çµ‚çš„ top_k çµæœ
            final_results = candidates[:top_k]
            
            # æ ¼å¼åŒ–æœ€çµ‚çµæœ
            processed_results = []
            for i, result in enumerate(final_results):
                metadata = result.get('metadata', {})
                processed_results.append({
                    'rank': i + 1,
                    'content': result['content'],
                    'metadata': metadata,
                    'filename': metadata.get('filename', 'Unknown'),
                    'source': metadata.get('source', 'Unknown'),
                    'chunk_id': metadata.get('chunk_id', 0),
                    'total_chunks': metadata.get('total_chunks', 1),
                    'distance_score': round(result.get('distance_score', -1), 4),
                    'rerank_score': round(result.get('rerank_score', -1), 4),
                })
            
            logging.info(f"âœ… æœ€çµ‚è¿”å› {len(processed_results)} å€‹çµæœ")
            return processed_results
            
        except Exception as e:
            logging.error(f"âŒ æœç´¢éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []

    def get_all_files(self) -> List[str]:
        """
        ç²å–è³‡æ–™åº«ä¸­æ‰€æœ‰æ–‡ä»¶çš„åˆ—è¡¨
        """
        try:
            all_results = self.vectordb.get(include=['metadatas'])
            metadatas = all_results.get('metadatas', [])
            
            filenames = set()
            for metadata in metadatas:
                if 'filename' in metadata:
                    filenames.add(metadata['filename'])
            
            return sorted(list(filenames))
            
        except Exception as e:
            logging.error(f"âŒ ç²å–æ–‡ä»¶åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
            
    def get_database_stats(self) -> Dict[str, Any]:
        """
        ç²å–è³‡æ–™åº«çµ±è¨ˆä¿¡æ¯
        """
        try:
            collection_count = self.vectordb._collection.count()
            all_files = self.get_all_files()
            
            stats = {
                'total_chunks': collection_count,
                'total_files': len(all_files),
                'files': all_files,
                'database_path': self.chroma_db_dir
            }
            
            return stats
            
        except Exception as e:
            logging.error(f"âŒ ç²å–çµ±è¨ˆä¿¡æ¯æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {}

def print_search_results(results: List[Dict[str, Any]], max_content_length: int = 300):
    """
    æ ¼å¼åŒ–è¼¸å‡ºæœç´¢çµæœ
    """
    if not results:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ç›¸é—œçµæœ")
        return
    
    print(f"\nğŸ¯ æ‰¾åˆ° {len(results)} å€‹ç›¸é—œçµæœ:")
    print("=" * 80)
    
    for result in results:
        print(f"ğŸ“ æ’å: {result['rank']}")
        
        # å„ªå…ˆé¡¯ç¤º Rerank åˆ†æ•¸
        if 'rerank_score' in result and result['rerank_score'] != -1:
            print(f"ğŸ¯ é‡æ’åºåˆ†æ•¸: {result['rerank_score']:.4f}")
        
        # é¡¯ç¤ºåˆå§‹çš„è·é›¢åˆ†æ•¸
        if 'distance_score' in result and result['distance_score'] != -1:
            print(f"ğŸ”— åˆå§‹è·é›¢åˆ†æ•¸: {result['distance_score']:.4f}")

        print(f"ğŸ“„ æ–‡ä»¶: {result['filename']}")
        print(f"ğŸ“‚ è·¯å¾‘: {result['source']}")
        
        if result['total_chunks'] > 1:
            print(f"ğŸ“‹ åˆ†å¡Š: {result['chunk_id'] + 1}/{result['total_chunks']}")
        
        content = result['content']
        if len(content) > max_content_length:
            content = content[:max_content_length] + "..."
            
        print(f"ğŸ“ å…§å®¹é è¦½:")
        print(f"   {content}")
        print("-" * 80)

def interactive_query_mode(query_system: VectorQuerySystem):
    """
    äº’å‹•å¼æŸ¥è©¢æ¨¡å¼
    """
    print("\nğŸš€ é€²å…¥äº’å‹•å¼æŸ¥è©¢æ¨¡å¼")
    print("è¼¸å…¥ 'help' æŸ¥çœ‹å¯ç”¨å‘½ä»¤")
    print("è¼¸å…¥ 'exit' æˆ– 'quit' é€€å‡ºç¨‹å¼")
    print("=" * 50)
    
    while True:
        try:
            user_input = input("\nğŸ” è«‹è¼¸å…¥æŸ¥è©¢ > ").strip()
            
            if not user_input:
                continue
            
            # è™•ç†ç‰¹æ®Šå‘½ä»¤
            if user_input.lower() in ['exit', 'quit', 'q']:
                print("ğŸ‘‹ å†è¦‹ï¼")
                break
            
            elif user_input.lower() == 'help':
                print("""
ğŸ“– å¯ç”¨å‘½ä»¤:
 - ç›´æ¥è¼¸å…¥æ–‡å­—é€²è¡Œèªç¾©æœç´¢
 - 'files' - é¡¯ç¤ºè³‡æ–™åº«ä¸­æ‰€æœ‰æ–‡ä»¶
 - 'stats' - é¡¯ç¤ºè³‡æ–™åº«çµ±è¨ˆä¿¡æ¯  
 - 'help' - é¡¯ç¤ºæ­¤å¹«åŠ©ä¿¡æ¯
 - 'exit' æˆ– 'quit' - é€€å‡ºç¨‹å¼
 
ğŸ”§ æœç´¢åƒæ•¸ï¼ˆå¯é¸ï¼‰:
 - åœ¨æŸ¥è©¢å¾ŒåŠ ä¸Š ':æ•¸å­—' æŒ‡å®šæœ€çµ‚çµæœæ•¸é‡ï¼Œä¾‹å¦‚: "æ©Ÿå™¨å­¸ç¿’:5"
 - åœ¨æŸ¥è©¢å¾ŒåŠ ä¸Š ':æ•¸å­—:æ•¸å­—' æŒ‡å®šæœ€çµ‚çµæœæ•¸é‡å’Œåˆå§‹å€™é¸é›†æ•¸é‡ï¼Œä¾‹å¦‚: "æ©Ÿå™¨å­¸ç¿’:5:100"
 - é è¨­è¿”å›5å€‹çµæœï¼Œå€™é¸é›†ç‚º50å€‹
         """)
                continue
            
            elif user_input.lower() == 'files':
                files = query_system.get_all_files()
                print(f"\nğŸ“ è³‡æ–™åº«åŒ…å« {len(files)} å€‹æ–‡ä»¶:")
                for i, filename in enumerate(files, 1):
                    print(f"  {i}. {filename}")
                continue
            
            elif user_input.lower() == 'stats':
                stats = query_system.get_database_stats()
                print(f"\nğŸ“Š è³‡æ–™åº«çµ±è¨ˆ:")
                print(f"  ç¸½æ–‡æª”ç‰‡æ®µ: {stats.get('total_chunks', 0)}")
                print(f"  ç¸½æ–‡ä»¶æ•¸: {stats.get('total_files', 0)}")
                print(f"  è³‡æ–™åº«è·¯å¾‘: {stats.get('database_path', 'Unknown')}")
                continue
            
            # è§£ææŸ¥è©¢åƒæ•¸
            top_k = 5
            initial_k = 50
            query_parts = user_input.split(':')
            user_query = query_parts[0].strip()
            if len(query_parts) >= 2 and query_parts[1].isdigit():
                top_k = int(query_parts[1])
            if len(query_parts) == 3 and query_parts[2].isdigit():
                initial_k = int(query_parts[2])

            # åŸ·è¡Œæœç´¢
            start_time = datetime.now()
            results = query_system.search(user_query, top_k=top_k, initial_k=initial_k)
            end_time = datetime.now()
            
            # é¡¯ç¤ºçµæœ
            print_search_results(results)
            print(f"\nâ±ï¸ æœç´¢è€—æ™‚: {(end_time - start_time).total_seconds():.3f} ç§’")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹å¼å·²ä¸­æ–·ï¼Œå†è¦‹ï¼")
            break
        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
            logging.error(f"äº’å‹•æ¨¡å¼éŒ¯èª¤: {e}")

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='å‘é‡è³‡æ–™åº«æŸ¥è©¢ç³»çµ± (æ•´åˆ Reranker)')
    parser.add_argument('--chroma_db_dir', type=str, default=r"D:\ArtificialIntelligenceCustomerService\vector_dbs\dev_dbs\tea_preview_list\cs_2048_co_10_min_1500",
                        help='å‘é‡è³‡æ–™åº«ç›®éŒ„ï¼Œé è¨­ç‚º "../vector_dbs/dev_dbs/"')
    parser.add_argument('--query', type=str, default=None,
                        help='ç›´æ¥åŸ·è¡ŒæŸ¥è©¢è€Œä¸é€²å…¥äº’å‹•æ¨¡å¼')
    parser.add_argument('--top_k', type=int, default=5,
                        help='æœ€çµ‚è¿”å›çµæœæ•¸é‡ï¼Œé è¨­ç‚º5')
    parser.add_argument('--initial_k', type=int, default=50,
                        help='åˆå§‹å‘é‡æœç´¢çš„å€™é¸é›†æ•¸é‡ï¼Œé è¨­ç‚º50')
    parser.add_argument('--embedding_model', type=str, 
                        default="BAAI/bge-m3",
                        help='åµŒå…¥æ¨¡å‹è·¯å¾‘')
    parser.add_argument('--reranker_model', type=str, 
                        default="BAAI/bge-reranker-v2-m3",
                        help='Reranker æ¨¡å‹è·¯å¾‘')
    
    args = parser.parse_args()
    
    try:
        print("ğŸš€ åˆå§‹åŒ–å‘é‡æŸ¥è©¢ç³»çµ±...")
        query_system = VectorQuerySystem(
            chroma_db_dir=args.chroma_db_dir,
            embedding_model_name=args.embedding_model,
            reranker_model_name=args.reranker_model
        )
        
        stats = query_system.get_database_stats()
        print(f"\nğŸ“Š è³‡æ–™åº«è¼‰å…¥æˆåŠŸ!")
        print(f"  ğŸ“ æ–‡æª”ç‰‡æ®µæ•¸: {stats.get('total_chunks', 0)}")
        print(f"  ğŸ“„ æ–‡ä»¶ç¸½æ•¸: {stats.get('total_files', 0)}")
        
        if args.query:
            print(f"\nğŸ” åŸ·è¡ŒæŸ¥è©¢: '{args.query}'")
            results = query_system.search(args.query, top_k=args.top_k, initial_k=args.initial_k)
            print_search_results(results)
        else:
            interactive_query_mode(query_system)
            
    except FileNotFoundError as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        print("è«‹ç¢ºä¿å·²ç¶“åŸ·è¡Œæ–‡æª”å‘é‡åŒ–ç¨‹å¼å»ºç«‹è³‡æ–™åº«")
    except Exception as e:
        print(f"âŒ ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
        logging.error(f"ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")

if __name__ == "__main__":
    main()