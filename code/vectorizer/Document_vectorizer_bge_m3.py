import os
import traceback
import logging
import argparse
from typing import Optional
import gc
from collections import defaultdict
import glob

# è¨­å®šPyTorchè¨˜æ†¶é«”å„ªåŒ–ç’°å¢ƒè®Šæ•¸
import torch
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# å‡è¨­é€™äº›å·¥å…·é¡åˆ¥éƒ½å·²å®šç¾©
from utils.execution_logger import ExecutionLogger
from utils.vision_processor import VisionProcessor
from utils.document_reader import DocumentReader
from utils.text_splitter import TextSplitter
from utils.summarizer import Summarizer
from utils.vectorDB_manager import VectorDBManager
from FileFormatAnalyzer import FileFormatAnalyzer

# è¨­å®š logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def process_file(file_path: str, document_reader: DocumentReader, text_splitter: TextSplitter,
                 vectordb_manager: VectorDBManager, execution_logger: ExecutionLogger, summarizer: Optional[Summarizer] = None) -> bool:
    """
    è™•ç†å–®å€‹æ–‡ä»¶ï¼šè®€å–ã€åˆ†å‰²ã€æ‘˜è¦(å¯é¸)ã€åŠ å…¥å‘é‡è³‡æ–™åº«ã€‚
    """
    try:
        logging.info(f"ğŸ“„ è™•ç†æª”æ¡ˆï¼š{file_path}")
    
        raw_text = document_reader.load_document(file_path)
        
        if raw_text:
            print("="*40)
            print(f"æˆåŠŸå¾ {file_path} è®€å–åˆ°çš„å…§å®¹ï¼š")
            print(raw_text[:500] + "...") # åªåˆ—å°å‰500å€‹å­—å…ƒä»¥é¿å…å…§å®¹éé•·
            print("="*40)

        if not raw_text:
            error_msg = "ç„¡æ³•è®€å–æ–‡ä»¶æˆ–æ–‡ä»¶å…§å®¹ç‚ºç©º"
            execution_logger.log_error(error_msg, file_path)
            logging.error(f"âŒ ç„¡æ³•è®€å–æ–‡ä»¶æˆ–æ–‡ä»¶å…§å®¹ç‚ºç©ºï¼š{file_path}")
            return False
            
        logging.info(f"ğŸ“„ æ–‡ä»¶è¼‰å…¥æˆåŠŸï¼Œå…§å®¹é•·åº¦ï¼š{len(raw_text)} å­—å…ƒ")
            
        logging.info(f"ğŸ”„ é–‹å§‹æ–‡æœ¬åˆ†å‰²è™•ç†...")
        documents = text_splitter.split_text(raw_text, file_path)
    
        if summarizer:
            logging.info("ğŸ“ é–‹å§‹å°æ¯å€‹æ–‡æœ¬å¡Šç”Ÿæˆæ‘˜è¦...")
            documents_with_summary = []
            for doc in documents:
                summary = summarizer.summarize_chunk(doc.page_content)
                doc.metadata["summary"] = summary
            
                documents_with_summary.append(doc)
                logging.info(f"å·²ç”Ÿæˆæ‘˜è¦ï¼Œæ‘˜è¦é•·åº¦ï¼š{len(summary)} å­—å…ƒ")
            documents_to_add = documents_with_summary
        else:
            documents_to_add = documents

        logging.info(f"ğŸ”„ é–‹å§‹å‘é‡åŒ–ä¸¦åŠ å…¥è³‡æ–™åº«...")
        vectordb_manager.add_documents(documents_to_add)
        
        logging.info(f"âœ… æ–‡ä»¶è™•ç†å®Œæˆï¼š{file_path}")
        
        return True

    except Exception as e:
        error_msg = f"è™•ç†æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
        execution_logger.log_error(error_msg, file_path)
        logging.error(f"âš ï¸ è™•ç† {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}", exc_info=True)
        traceback.print_exc()
        return False


def process_subdirectories(base_source_dir: str, base_chroma_db_dir: str, summarize_enabled: bool, single_db_mode: bool):
    """
    è™•ç†æ‰€æœ‰å­ç›®éŒ„ä¸¦æ ¹æ“š single_db_mode å»ºç«‹å‘é‡è³‡æ–™åº«
    """
    # æ±ºå®šæœ€çµ‚çš„å‘é‡è³‡æ–™åº«ç›®éŒ„
    if single_db_mode:
        # å–®ä¸€æ¨¡å¼: æ‰€æœ‰å­ç›®éŒ„æ–‡ä»¶éƒ½åŒ¯å…¥åˆ°ä¸€å€‹è³‡æ–™åº«
        target_db_dir = base_chroma_db_dir
        logging.info(f"**å•Ÿç”¨å–®ä¸€è³‡æ–™åº«æ¨¡å¼**ï¼šæ‰€æœ‰æ–‡ä»¶å°‡åŒ¯å…¥åˆ° {target_db_dir}")
        
        # å°‹æ‰¾æ‰€æœ‰å­ç›®éŒ„ä¸­çš„æ–‡ä»¶
        all_subdirs = [os.path.join(base_source_dir, d) for d in os.listdir(base_source_dir) if os.path.isdir(os.path.join(base_source_dir, d))]
        if not all_subdirs:
            logging.error(f"åœ¨ {base_source_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•å­ç›®éŒ„ã€‚")
            return
            
        # ç‚ºäº†ä¿æŒèˆ‡ process_single_directory çš„é‚è¼¯ä¸€è‡´ï¼Œæˆ‘å€‘å°‡æ‰€æœ‰å­ç›®éŒ„è¦–ç‚ºä¸€å€‹è™›æ“¬çš„ã€Œå–®ä¸€ç›®éŒ„ã€
        # ä½†å¯¦éš›éæ­·æ™‚éœ€è¦éæ­¸å°‹æ‰¾æ‰€æœ‰æ–‡ä»¶
        process_single_directory(base_source_dir, target_db_dir, summarize_enabled, is_recursive=True)

    else:
        # åˆ†æ•£æ¨¡å¼: æ¯å€‹å­ç›®éŒ„å»ºç«‹ä¸€å€‹å‘é‡è³‡æ–™åº«
        subdirectories = [d for d in os.listdir(base_source_dir) if os.path.isdir(os.path.join(base_source_dir, d))]

        if not subdirectories:
            logging.error(f"åœ¨ {base_source_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•å­ç›®éŒ„ã€‚è«‹ç¢ºèªæ­¤è·¯å¾‘ä¸‹åŒ…å«å„ç³»æ‰€çš„è³‡æ–™å¤¾ã€‚")
            return

        for subdir_name in subdirectories:
            source_dir = os.path.join(base_source_dir, subdir_name)
            chroma_db_dir = os.path.join(base_chroma_db_dir, subdir_name)

            logging.info(f"\n======== é–‹å§‹è™•ç†å­ç›®éŒ„ï¼š{source_dir} ========")

            process_single_directory(source_dir, chroma_db_dir, summarize_enabled, is_recursive=False) # é€™è£¡åªè™•ç†å–®ä¸€å±¤ç´š

            logging.info(f"======== å­ç›®éŒ„ {subdir_name} è™•ç†å®Œæˆ ========")

def process_single_directory(source_dir: str, chroma_db_dir: str, summarize_enabled: bool, is_recursive: bool = False):
    """
    è™•ç†å–®ä¸€ç›®éŒ„ï¼ˆæˆ–éæ­¸è™•ç†åŸºåº•ç›®éŒ„ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼‰ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«
    :param is_recursive: æ˜¯å¦éæ­¸éæ­· source_dir ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
    """
    if not os.path.exists(chroma_db_dir):
        os.makedirs(chroma_db_dir)
        logging.info(f"å‰µå»ºäº†å‘é‡è³‡æ–™åº«ç›®éŒ„: {chroma_db_dir}")

    execution_log_path = os.path.join(chroma_db_dir, "execution_log.txt")
    execution_logger = ExecutionLogger(execution_log_path)
    
    if not os.path.exists(source_dir):
        error_msg = f"æºæ–‡ä»¶ç›®éŒ„ä¸å­˜åœ¨: {source_dir}"
        execution_logger.log_error(error_msg)
        logging.error(error_msg)
        return
    
    input_files = []
    supported_extensions = ['.pdf', '.html', '.htm', '.txt', '.docx','.json', '.csv', '.doc', '.odt']
    
    # æ–°å¢ä¸€å€‹å­—å…¸ä¾†è¿½è¹¤æ–‡ä»¶æ ¼å¼çš„çµ±è¨ˆ
    file_stats = {'valid': defaultdict(int), 'invalid': defaultdict(int)}
    
    # æ ¹æ“š is_recursive æ±ºå®šéæ­·æ–¹å¼
    if is_recursive:
        # éæ­¸æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶
        search_path = os.path.join(source_dir, "**/*")
        file_list = glob.glob(search_path, recursive=True)
    else:
        # åªæŸ¥æ‰¾ç•¶å‰ç›®éŒ„ä¸‹çš„æ–‡ä»¶ (process_subdirectories ä¸­æœƒå°æ¯å€‹å­ç›®éŒ„èª¿ç”¨)
        file_list = [os.path.join(source_dir, f) for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]

    for file_path in file_list:
        if os.path.basename(file_path).startswith('.'):
            continue
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if file_ext in supported_extensions:
            input_files.append(file_path)
        else:
            execution_logger.log_skip(f"è·³éä¸æ”¯æ´çš„æ–‡ä»¶æ ¼å¼: {file_ext}", file_path)
    
    if not input_files:
        error_msg = f"åœ¨ {source_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•æ”¯æ´çš„æ–‡ä»¶"
        execution_logger.log_error(error_msg)
        logging.error(error_msg)
        return
    
    logging.info(f"æ‰¾åˆ° {len(input_files)} å€‹å¾…è™•ç†æ–‡ä»¶")
    logging.info(f"æ”¯æ´çš„æ ¼å¼: {', '.join(supported_extensions)}")
    
    try:
        vision_processor = VisionProcessor()
        document_reader = DocumentReader(vision_processor=vision_processor, execution_logger=execution_logger)
        text_splitter = TextSplitter(chunk_size=2048, chunk_overlap=100, min_chunk_size=1500)
        vectordb_manager = VectorDBManager(chroma_db_dir)
        
        if summarize_enabled:
            summarizer = Summarizer()
        else:
            summarizer = None
            
    except Exception as e:
        error_msg = f"åˆå§‹åŒ–çµ„ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        execution_logger.log_error(error_msg)
        logging.error(error_msg)
        return
    
    success_count = 0
    fail_count = 0
    skip_count = 0
    
    # åœ¨è™•ç†è¿´åœˆå‰ï¼Œå…ˆç²å–æ‰€æœ‰å·²è™•ç†éçš„æ–‡ä»¶è·¯å¾‘
    try:
        processed_file_paths = vectordb_manager.get_processed_paths()
        logging.info(f"å·²å¾è³‡æ–™åº«è®€å– {len(processed_file_paths)} å€‹å·²è™•ç†æ–‡ä»¶è·¯å¾‘ã€‚")
    except Exception as e:
        logging.warning(f"ç„¡æ³•å¾è³‡æ–™åº«è®€å–å·²è™•ç†æ–‡ä»¶è·¯å¾‘ï¼Œå°‡å¾é ­è™•ç†æ‰€æœ‰æ–‡ä»¶ã€‚éŒ¯èª¤: {e}")
        processed_file_paths = set()

    for i, file_path in enumerate(input_files, 1):
        logging.info(f"è™•ç†é€²åº¦: {i}/{len(input_files)}")
        
        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å·²è™•ç†é
        if file_path in processed_file_paths:
            logging.info(f"â­ï¸ æ–‡ä»¶å·²å­˜åœ¨æ–¼è³‡æ–™åº«ä¸­ï¼Œè·³éè™•ç†: {file_path}")
            execution_logger.log_skip(f"æ–‡ä»¶å·²å­˜åœ¨æ–¼è³‡æ–™åº«ä¸­ï¼Œè·³éè™•ç†", file_path)
            skip_count += 1
            continue

        # é€™è£¡éœ€è¦ä¿®æ”¹ process_file å‡½æ•¸ï¼Œè®“å®ƒèƒ½è¿”å›æˆåŠŸèˆ‡å¦
        success = process_file(
            file_path=file_path,
            document_reader=document_reader,
            text_splitter=text_splitter,
            vectordb_manager=vectordb_manager,
            execution_logger=execution_logger,
            summarizer=summarizer
        )
        
        file_ext = os.path.splitext(file_path)[1].lower()
        if success:
            success_count += 1
            file_stats['valid'][file_ext] += 1
        else:
            fail_count += 1
            file_stats['invalid'][file_ext] += 1
            
        torch.cuda.empty_cache()
        gc.collect()
    
    # è¨˜éŒ„æ–‡ä»¶æ ¼å¼çµ±è¨ˆ
    execution_logger.write_to_log("\n--- æ–‡ä»¶æ ¼å¼è™•ç†çµ±è¨ˆ ---")
    for ext in supported_extensions:
        valid_count = file_stats['valid'][ext]
        invalid_count = file_stats['invalid'][ext]
        execution_logger.write_to_log(f"æ ¼å¼ {ext}: æœ‰æ•ˆæ–‡ä»¶æ•¸={valid_count}, ç„¡æ•ˆæ–‡ä»¶æ•¸={invalid_count}")
    
    execution_logger.finalize_log(success_count, fail_count, skip_count)
    
    logging.info(f"ğŸ‰ è™•ç†å®Œæˆï¼")
    logging.info(f"âœ… æˆåŠŸè™•ç†: {success_count} å€‹æ–‡ä»¶")
    logging.info(f"âŒ è™•ç†å¤±æ•—: {fail_count} å€‹æ–‡ä»¶")
    logging.info(f"â­ï¸ è·³éæ–‡ä»¶: {skip_count} å€‹æ–‡ä»¶")
    logging.info(f"ğŸ“Š å‘é‡è³‡æ–™åº«ä½ç½®: {chroma_db_dir}")
    logging.info(f"ğŸ“‹ è©³ç´°åŸ·è¡Œæ—¥èªŒ: {execution_log_path}")


def main():
    """ä¸»å‡½æ•¸"""
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    if torch.cuda.is_available():
        logging.info(f"CUDAå¯ç”¨ã€‚ç™¼ç¾ {torch.cuda.device_count()} å€‹GPUè¨­å‚™")
    
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            logging.info(f"GPU {i}: {props.name}, è¨˜æ†¶é«”: {props.total_memory / 1024**3:.2f} GB")
            logging.info(f"ç›®å‰ä½¿ç”¨çš„è¨˜æ†¶é«”: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB")
            logging.info(f"ç›®å‰ä¿ç•™çš„è¨˜æ†¶é«”: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB")
    else:
        logging.warning("CUDAä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨CPUé‹è¡Œ")
    
    torch.cuda.empty_cache()
    gc.collect()

    
    parser = argparse.ArgumentParser(description='æ–‡ä»¶å‘é‡åŒ–ç³»çµ± - å°‡åŸå§‹æ–‡ä»¶å…§å®¹å‘é‡åŒ–ä¸¦å­˜å…¥è³‡æ–™åº«')
    parser.add_argument('--source_dir', type=str, default=r"C:\Users\IFangLab\Desktop\data_conclusion", 
                        help='åŒ…å«å„ç³»æ‰€è³‡æ–™å¤¾çš„åŸºåº•æºæ–‡ä»¶ç›®éŒ„')
    parser.add_argument('--chroma_db_dir', type=str, default="../../tmp_db",
                        help='å‘é‡è³‡æ–™åº«çš„åŸºåº•å­˜å„²ç›®éŒ„ã€‚è‹¥ç‚ºåˆ†æ•£æ¨¡å¼ï¼Œæ¯å€‹å­ç›®éŒ„æœƒåœ¨æ­¤è™•å»ºç«‹å°æ‡‰çš„DBï¼›è‹¥ç‚ºå–®ä¸€æ¨¡å¼ï¼Œæ‰€æœ‰æ–‡ä»¶å°‡åŒ¯å…¥åˆ°æ­¤ç›®éŒ„ä¸‹çš„DBã€‚')
    parser.add_argument('--summarize', action='store_true',
                        help='å•Ÿç”¨å°æ¯å€‹chunké€²è¡Œæ‘˜è¦çš„åŠŸèƒ½')
    parser.add_argument('--single_db', action='store_true',
                        help='å•Ÿç”¨å–®ä¸€è³‡æ–™åº«æ¨¡å¼ã€‚å¦‚æœè¨­å®šæ­¤æ——æ¨™ï¼Œæ‰€æœ‰å­ç›®éŒ„ä¸­çš„æ–‡ä»¶å°‡è¢«åŒ¯å…¥åˆ°ä¸€å€‹å–®ä¸€çš„å‘é‡è³‡æ–™åº«ï¼ˆ--chroma_db_dirï¼‰ã€‚å¦å‰‡ï¼Œæ¯å€‹å­ç›®éŒ„å°‡æœ‰è‡ªå·±çš„è³‡æ–™åº«ã€‚')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.source_dir):
        logging.error(f"æºæ–‡ä»¶åŸºåº•ç›®éŒ„ä¸å­˜åœ¨: {args.source_dir}")
        return
    
    file_analyzer = FileFormatAnalyzer(args.source_dir)
    file_analyzer.analyze_folder()
    file_analyzer.generate_report()

    process_subdirectories(
        base_source_dir=args.source_dir,
        base_chroma_db_dir=args.chroma_db_dir,
        summarize_enabled=args.summarize,
        single_db_mode=args.single_db # å‚³éæ–°åƒæ•¸
    )

if __name__ == "__main__":
    main()