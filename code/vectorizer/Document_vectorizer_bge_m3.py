import os
import traceback
import logging
import argparse
from typing import Optional
import gc

# è¨­å®šPyTorchè¨˜æ†¶é«”å„ªåŒ–ç’°å¢ƒè®Šæ•¸
import torch
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

from utils.execution_logger import ExecutionLogger
from utils.vision_processor import VisionProcessor
from utils.document_reader import DocumentReader
from utils.text_splitter import TextSplitter
from utils.summarizer import Summarizer
from utils.vectorDB_manager import VectorDBManager
from FileFormatAnalyzer import FileFormatAnalyzer

# è¨­å®š logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def process_file(file_path: str, document_reader: DocumentReader, text_splitter: TextSplitter,
                 vectordb_manager: VectorDBManager, execution_logger: ExecutionLogger, summarizer: Optional[Summarizer] = None):
    """
    è™•ç†å–®å€‹æ–‡ä»¶ï¼šè®€å–ã€åˆ†å‰²ã€æ‘˜è¦(å¯é¸)ã€åŠ å…¥å‘é‡è³‡æ–™åº«ã€‚
    """
    try:
        logging.info(f"ğŸ“„ è™•ç†æª”æ¡ˆï¼š{file_path}")
        raw_text = document_reader.load_document(file_path)
        
        if raw_text:
            print("="*40)
            print(f"æˆåŠŸå¾ {file_path} è®€å–åˆ°çš„å…§å®¹ï¼š")
            print(raw_text[:500] + "...") # åªåˆ—å°å‰500å€‹å­—å…ƒä»¥é¿å…å…§å®¹éé•·
            print("="*40)

        if not raw_text:
            error_msg = "ç„¡æ³•è®€å–æ–‡ä»¶æˆ–æ–‡ä»¶å…§å®¹ç‚ºç©º"
            execution_logger.log_error(error_msg, file_path)
            logging.error(f"âŒ ç„¡æ³•è®€å–æ–‡ä»¶æˆ–æ–‡ä»¶å…§å®¹ç‚ºç©ºï¼š{file_path}")
            return False
            
        logging.info(f"ğŸ“„ æ–‡ä»¶è¼‰å…¥æˆåŠŸï¼Œå…§å®¹é•·åº¦ï¼š{len(raw_text)} å­—å…ƒ")
            
        logging.info(f"ğŸ”„ é–‹å§‹æ–‡æœ¬åˆ†å‰²è™•ç†...")
        documents = text_splitter.split_text(raw_text, file_path)
        
        if summarizer:
            logging.info("ğŸ“ é–‹å§‹å°æ¯å€‹æ–‡æœ¬å¡Šç”Ÿæˆæ‘˜è¦...")
            documents_with_summary = []
            for doc in documents:
                summary = summarizer.summarize_chunk(doc.page_content)
                doc.metadata["summary"] = summary
                documents_with_summary.append(doc)
                logging.info(f"å·²ç”Ÿæˆæ‘˜è¦ï¼Œæ‘˜è¦é•·åº¦ï¼š{len(summary)} å­—å…ƒ")
            documents_to_add = documents_with_summary
        else:
            documents_to_add = documents

        logging.info(f"ğŸ”„ é–‹å§‹å‘é‡åŒ–ä¸¦åŠ å…¥è³‡æ–™åº«...")
        vectordb_manager.add_documents(documents_to_add)
        
        logging.info(f"âœ… æ–‡ä»¶è™•ç†å®Œæˆï¼š{file_path}")
        return True

    except Exception as e:
        error_msg = f"è™•ç†æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
        execution_logger.log_error(error_msg, file_path)
        logging.error(f"âš ï¸ è™•ç† {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}", exc_info=True)
        traceback.print_exc()
        return False


def process_all_files(source_dir, chroma_db_dir, summarize_enabled: bool):
    """
    è™•ç†æ‰€æœ‰æ–‡ä»¶ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«
    """
    execution_logger = ExecutionLogger("execution_log.txt")
    
    if not os.path.exists(source_dir):
        error_msg = f"æºæ–‡ä»¶ç›®éŒ„ä¸å­˜åœ¨: {source_dir}"
        execution_logger.log_error(error_msg)
        logging.error(error_msg)
        return
    
    if not os.path.exists(chroma_db_dir):
        os.makedirs(chroma_db_dir)
        logging.info(f"å‰µå»ºäº†å‘é‡è³‡æ–™åº«ç›®éŒ„: {chroma_db_dir}")
    
    input_files = []
    supported_extensions = ['.pdf', '.html', '.htm', '.txt', '.docx', '.json', '.csv', '.doc', '.odt']
    #supported_extensions = ['.pdf', '.html', '.htm', '.txt', '.docx', '.jpg', '.jpeg', '.png', '.json', '.csv', '.doc', '.odt']
    skipped_files = []
    
    for root, _, files in os.walk(source_dir):
        for file in files:
            if file.startswith('.'):
                continue
            file_ext = os.path.splitext(file)[1].lower()
            file_path = os.path.join(root, file)
            
            if file_ext in supported_extensions:
                input_files.append(file_path)
            else:
                skipped_files.append(file_path)
                execution_logger.log_skip(f"è·³éä¸æ”¯æ´çš„æ–‡ä»¶æ ¼å¼: {file_ext}", file_path)
    
    if not input_files:
        error_msg = f"åœ¨ {source_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•æ”¯æ´çš„æ–‡ä»¶"
        execution_logger.log_error(error_msg)
        logging.error(error_msg)
        return
    
    logging.info(f"æ‰¾åˆ° {len(input_files)} å€‹å¾…è™•ç†æ–‡ä»¶")
    logging.info(f"è·³é {len(skipped_files)} å€‹ä¸æ”¯æ´çš„æ–‡ä»¶")
    logging.info(f"æ”¯æ´çš„æ ¼å¼: {', '.join(supported_extensions)}")
    
    try:
        vision_processor = VisionProcessor()
        document_reader = DocumentReader(vision_processor=vision_processor, execution_logger=execution_logger)
        text_splitter = TextSplitter(chunk_size=2048, chunk_overlap=50, min_chunk_size=400)
        vectordb_manager = VectorDBManager(chroma_db_dir)
        
        if summarize_enabled:
            summarizer = Summarizer()
        else:
            summarizer = None
            
    except Exception as e:
        error_msg = f"åˆå§‹åŒ–çµ„ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        execution_logger.log_error(error_msg)
        logging.error(error_msg)
        return
    
    success_count = 0
    fail_count = 0
    skip_count = len(skipped_files)
     
    for i, file_path in enumerate(input_files, 1):
        logging.info(f"è™•ç†é€²åº¦: {i}/{len(input_files)}")
        
        success = process_file(
            file_path=file_path,
            document_reader=document_reader,
            text_splitter=text_splitter,
            vectordb_manager=vectordb_manager,
            execution_logger=execution_logger,
            summarizer=summarizer
        )
        
        if success:
            success_count += 1
        else:
            fail_count += 1
            
        torch.cuda.empty_cache()
        gc.collect()
    
    execution_logger.finalize_log(success_count, fail_count, skip_count)
    
    logging.info(f"ğŸ‰ è™•ç†å®Œæˆï¼")
    logging.info(f"âœ… æˆåŠŸè™•ç†: {success_count} å€‹æ–‡ä»¶")
    logging.info(f"âŒ è™•ç†å¤±æ•—: {fail_count} å€‹æ–‡ä»¶")
    logging.info(f"â­ï¸ è·³éæ–‡ä»¶: {skip_count} å€‹æ–‡ä»¶")
    logging.info(f"ğŸ“Š å‘é‡è³‡æ–™åº«ä½ç½®: {chroma_db_dir}")
    logging.info(f"ğŸ“‹ è©³ç´°åŸ·è¡Œæ—¥èªŒ: execution_log.txt")


def main():
    """ä¸»å‡½æ•¸"""
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    if torch.cuda.is_available():
        logging.info(f"CUDAå¯ç”¨ã€‚ç™¼ç¾ {torch.cuda.device_count()} å€‹GPUè¨­å‚™")
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            logging.info(f"GPU {i}: {props.name}, è¨˜æ†¶é«”: {props.total_memory / 1024**3:.2f} GB")
            logging.info(f"ç›®å‰ä½¿ç”¨çš„è¨˜æ†¶é«”: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB")
            logging.info(f"ç›®å‰ä¿ç•™çš„è¨˜æ†¶é«”: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB")
    else:
        logging.warning("CUDAä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨CPUé‹è¡Œ")
    
    torch.cuda.empty_cache()
    gc.collect()

    parser = argparse.ArgumentParser(description='æ–‡ä»¶å‘é‡åŒ–ç³»çµ± - å°‡åŸå§‹æ–‡ä»¶å…§å®¹å‘é‡åŒ–ä¸¦å­˜å…¥è³‡æ–™åº«')
    parser.add_argument('--source_dir', type=str, default=r"C:\Users\IFangLab\Desktop\data_conclusion\æ•™å‹™è™•", 
                   help='æºæ–‡ä»¶ç›®éŒ„ï¼Œé»˜èªç‚º "../data"')
    # parser.add_argument('--source_dir', type=str, default="../../data", 
    #               help='æºæ–‡ä»¶ç›®éŒ„ï¼Œé»˜èªç‚º "../data"')
    parser.add_argument('--chroma_db_dir', type=str, default="../../tmp_db",
                      help='å‘é‡è³‡æ–™åº«å­˜å„²ç›®éŒ„ï¼Œé»˜èªç‚º "../tmp_db"')
    parser.add_argument('--summarize', action='store_true',
                      help='å•Ÿç”¨å°æ¯å€‹chunké€²è¡Œæ‘˜è¦çš„åŠŸèƒ½')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.source_dir):
        logging.error(f"æºæ–‡ä»¶ç›®éŒ„ä¸å­˜åœ¨: {args.source_dir}")
        return
    
    file_analyzer = FileFormatAnalyzer(args.source_dir)
    file_analyzer.analyze_folder()
    file_analyzer.generate_report()
    
    process_all_files(
        source_dir=args.source_dir,
        chroma_db_dir=args.chroma_db_dir,
        summarize_enabled=args.summarize
    )

if __name__ == "__main__":
    main()